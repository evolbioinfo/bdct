import re
import pandas as pd
import logging
import os

# Set up basic logging for this script
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Summarize results from the new BD-Skyline test (bdsky_test.py log files).")
    parser.add_argument('--logs', nargs='+', type=str,
                        help="BD-Skyline test results log files generated by bdsky_test.py")
    parser.add_argument('--tab', type=str, help="Output summary table file (TSV format)")
    parser.add_argument('--verbose', action='store_true', help="Enable verbose logging for debugging parsing issues")
    params = parser.parse_args()

    # Define the columns specifically for the new bdsky_test.py output
    df = pd.DataFrame(
        columns=['model', 'tree/forest', 'id', 'evidence_type', 'BDSKY_evidence', 'evidence_binary',
                 'internal_u_stat', 'external_u_stat', 'internal_pval', 'external_pval',
                 'T_value', 'num_tips', 'result', 'test_type'])

    if not params.logs:
        logging.error("No log files provided. Please use --logs to specify input log files.")
    else:
        for log_path in params.logs:  # Renamed 'log' to 'log_path' to avoid confusion with log content
            logging.debug(f"Processing log file: {log_path}")
            try:
                with open(log_path, 'r') as f:
                    content = f.read()

                # Extract tree ID from filename - assuming filename contains a numeric ID
                # e.g., tree_001.log -> id=1
                match_id = re.findall(r'(\d+)', os.path.basename(log_path))
                i = int(match_id[-1]) if match_id else 0  # Use 0 or some default if no ID found

                # Determine model type from log content
                model = 'UNKNOWN'
                # Check for "New BD-Skyline test results" or "NEW SKY test"
                if 'New BD-Skyline test results' in content or 'NEW SKY test: Evidence of BD-Skyline model detected' in content:
                    model = 'BDSKY'

                data_type = 'tree'  # This script processes individual tree logs

                # This script is specifically for the 'new_strategy' or 'balanced' test
                test_type = 'new_strategy'  # Based on log content: "new strategy" or "new strategy comparison"

                # Initialize variables for the current log file
                evidence_binary_uncorrected = 0
                evidence_binary_bonferroni = 0
                internal_u_stat = float('nan')
                external_u_stat = float('nan')
                internal_pval = float('nan')
                external_pval = float('nan')
                T_value = float('nan')
                num_tips = 0

                # Parse total tips
                tips_match = re.search(r'Total tips in tree:\s*(\d+)', content)
                if tips_match:
                    num_tips = int(tips_match.group(1))

                # Parse T value
                t_match = re.search(r'Time \(T\) based on \d+ tips:\s*([\d.]+)', content)
                if t_match:
                    T_value = float(t_match.group(1))

                # Parse internal branches results - CORRECTED REGEX to handle 'comparison'
                internal_section_match = re.search(
                    r'Internal branches \(new strategy(?: comparison)?\):\s*'  # Added (?: comparison)?
                    r'.*?T used = ([\d.]+)\s*'
                    r'.*?Early subtree interval: .*?\n'
                    r'.*?Late subtree interval: .*?\n'
                    r'.*?Mann-Whitney U statistic: ([\d.]+)\s*'
                    r'.*?p-value: ([\d.e-]+)', content, re.DOTALL
                )
                if internal_section_match:
                    # Note: T used is group(1), U statistic is group(2), p-value is group(3)
                    # We are only storing U stat and p-value here. T_value is parsed separately.
                    internal_u_stat = float(internal_section_match.group(2))
                    internal_pval = float(internal_section_match.group(3))
                else:
                    logging.debug(f"No internal branch section found in {log_path}")

                # Parse external branches results - CORRECTED REGEX to handle 'comparison'
                external_section_match = re.search(
                    r'External branches \(new strategy(?: comparison)?\):\s*'  # Added (?: comparison)?
                    r'.*?T used = ([\d.]+)\s*'
                    r'.*?Early subtree interval: .*?\n'
                    r'.*?Late subtree interval: .*?\n'
                    r'.*?Mann-Whitney U statistic: ([\d.]+)\s*'
                    r'.*?p-value: ([\d.e-]+)', content, re.DOTALL
                )
                if external_section_match:
                    external_u_stat = float(external_section_match.group(2))
                    external_pval = float(external_section_match.group(3))
                else:
                    logging.debug(f"No external branch section found in {log_path}")

                # Determine overall evidence based on the summary lines
                evidence_uncorrected_line = re.search(r'Evidence of skyline model \(uncorrected\):\s*(Yes|No)', content)
                evidence_bonferroni_line = re.search(r'Evidence of skyline model \(Bonferroni\):\s*(Yes|No)', content)

                if evidence_uncorrected_line:
                    evidence_binary_uncorrected = 1 if evidence_uncorrected_line.group(1) == 'Yes' else 0

                if evidence_bonferroni_line:
                    evidence_binary_bonferroni = 1 if evidence_bonferroni_line.group(1) == 'Yes' else 0

                # Fallback if specific summary lines are not found (e.g., if parsing was partial)
                alpha = 0.05
                bonferroni_alpha = alpha / 2  # Hardcoded as in bdsky_test.py

                if pd.isna(internal_pval) and pd.isna(external_pval):
                    logging.warning(f"No p-values found for {log_path}. Cannot determine significance.")
                else:
                    if (not pd.isna(internal_pval) and internal_pval < alpha) or \
                            (not pd.isna(external_pval) and external_pval < alpha):
                        evidence_binary_uncorrected = 1

                    if (not pd.isna(internal_pval) and internal_pval < bonferroni_alpha) or \
                            (not pd.isna(external_pval) and external_pval < bonferroni_alpha):
                        evidence_binary_bonferroni = 1

                # Set evidence text based on binary values
                evidence_text_uncorrected = "Yes" if evidence_binary_uncorrected else "No"
                evidence_text_bonferroni = "Yes" if evidence_binary_bonferroni else "No"

                # Debug logging
                if params.verbose:
                    logging.debug(f"File: {log_path}")
                    logging.debug(f"Model: {model}")
                    logging.debug(f"ID: {i}")
                    logging.debug(f"Num Tips: {num_tips}")
                    logging.debug(f"T Value: {T_value:.4f}")
                    logging.debug(f"Internal U stat: {internal_u_stat:.4f}, Pval: {internal_pval:.6f}")
                    logging.debug(f"External U stat: {external_u_stat:.4f}, Pval: {external_pval:.6f}")
                    logging.debug(f"Evidence uncorrected: {evidence_text_uncorrected} ({evidence_binary_uncorrected})")
                    logging.debug(f"Evidence bonferroni: {evidence_text_bonferroni} ({evidence_binary_bonferroni})")

                # Create entries for both uncorrected and Bonferroni-corrected results
                for evidence_type, evidence_text, evidence_binary in [
                    ('uncorrected', evidence_text_uncorrected, evidence_binary_uncorrected),
                    ('bonferroni', evidence_text_bonferroni, evidence_binary_bonferroni)
                ]:
                    # Determine result classification
                    if model == 'BDSKY':
                        result = 'TP' if evidence_binary == 1 else 'FN'
                    # Add logic for other models if needed
                    # elif model in ['BDCT', 'BD']:
                    #     result = 'TN' if evidence_binary == 0 else 'FP'
                    else:  # For 'UNKNOWN' or other models where we don't define TP/TN/FP/FN this way
                        result = 'UNDEF'

                    # Add data to DataFrame
                    df.loc[f'{model}.{data_type}.{i}.{evidence_type}',
                    ['model', 'tree/forest', 'id', 'evidence_type', 'BDSKY_evidence', 'evidence_binary',
                     'internal_u_stat', 'external_u_stat', 'internal_pval', 'external_pval',
                     'T_value', 'num_tips', 'result', 'test_type']] = [
                        model, data_type, i, evidence_type, evidence_text, evidence_binary,
                        internal_u_stat, external_u_stat, internal_pval, external_pval,
                        T_value, num_tips, result, test_type
                    ]

            except Exception as e:
                logging.error(f"Error processing {log_path}: {e}")
                if params.verbose:
                    import traceback

                    logging.error(traceback.format_exc())
                continue

    # --- Global statistics ---
    # Ensure numeric columns are actually numeric before calculating statistics
    numeric_columns = ['evidence_binary', 'internal_u_stat', 'external_u_stat',
                       'internal_pval', 'external_pval', 'T_value', 'num_tips']
    for col in numeric_columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')  # Do not fillna(0) here, keep NaN for correct aggregation

    for evidence_type in ['uncorrected', 'bonferroni']:
        df_subset = df[df['evidence_type'] == evidence_type]

        TP = len(df_subset[df_subset['result'] == 'TP'])
        TN = len(df_subset[df_subset['result'] == 'TN'])
        FP = len(df_subset[df_subset['result'] == 'FP'])
        FN = len(df_subset[df_subset['result'] == 'FN'])

        print(f'Global ({evidence_type}):')
        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else float('nan')
        specificity = TN / (TN + FP) if (TN + FP) > 0 else float('nan')
        accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else float('nan')
        precision = TP / (TP + FP) if (TP + FP) > 0 else float('nan')
        print(f'\tTP={TP}, TN={TN}, FP={FP}, FN={FN}')
        print(f'\tSensitivity={sensitivity:.4f}, Specificity={specificity:.4f}')
        print(f'\tAccuracy={accuracy:.4f}, Precision={precision:.4f}')
        print('==============\n')

    # --- Statistics by model ---
    for evidence_type in ['uncorrected', 'bonferroni']:
        print(f'\n=== {evidence_type.upper()} RESULTS ===')
        df_evidence = df[df['evidence_type'] == evidence_type]

        for test_type in df_evidence['test_type'].unique():
            df_test_type = df_evidence[df_evidence['test_type'] == test_type]
            print(f'\n--- {test_type.upper()} TEST TYPE ---')

            for model in df_test_type['model'].unique():
                for data_type in df_test_type['tree/forest'].unique():
                    ddf = df_test_type[(df_test_type['model'] == model) & (df_test_type['tree/forest'] == data_type)]
                    if len(ddf) == 0:
                        continue

                    # Basic statistics
                    mean_evidence = ddf['evidence_binary'].mean()
                    mean_internal_u = ddf['internal_u_stat'].mean()
                    mean_external_u = ddf['external_u_stat'].mean()
                    mean_internal_pval = ddf['internal_pval'].mean()
                    mean_external_pval = ddf['external_pval'].mean()
                    mean_T = ddf['T_value'].mean()

                    num_detected = len(ddf[ddf['evidence_binary'] == 1])
                    percentage_detected = 100 * num_detected / len(ddf) if len(ddf) > 0 else 0.0

                    TP_model = len(ddf[ddf['result'] == 'TP'])
                    TN_model = len(ddf[ddf['result'] == 'TN'])
                    FP_model = len(ddf[ddf['result'] == 'FP'])
                    FN_model = len(ddf[ddf['result'] == 'FN'])

                    tips_min = ddf['num_tips'].min() if not ddf['num_tips'].isnull().all() else float('nan')
                    tips_mean = ddf['num_tips'].mean() if not ddf['num_tips'].isnull().all() else float('nan')
                    tips_max = ddf['num_tips'].max() if not ddf['num_tips'].isnull().all() else float('nan')

                    print(f'{model} on {data_type}s ({evidence_type}, {test_type}):')

                    sensitivity_model = TP_model / (TP_model + FN_model) if (TP_model + FN_model) > 0 else float('nan')
                    specificity_model = TN_model / (TN_model + FP_model) if (TN_model + FP_model) > 0 else float('nan')
                    accuracy_model = (TP_model + TN_model) / (TP_model + TN_model + FP_model + FN_model) if (
                                                                                                                    TP_model + TN_model + FP_model + FN_model) > 0 else float(
                        'nan')
                    precision_model = TP_model / (TP_model + FP_model) if (TP_model + FP_model) > 0 else float('nan')

                    print(f'\tTP={TP_model}, TN={TN_model}, FP={FP_model}, FN={FN_model}')
                    print(f'\tSensitivity={sensitivity_model:.4f}, Specificity={specificity_model:.4f}')
                    print(f'\tAccuracy={accuracy_model:.4f}, Precision={precision_model:.4f}')

                    print('--------Test metrics:')
                    print(f'\tSkyline detected\t{percentage_detected:.1f}%')
                    print(f'\tavg evidence rate\t{mean_evidence:.3f}')
                    print(f'\tavg internal U stat\t{mean_internal_u:.4f}')
                    print(f'\tavg external U stat\t{mean_external_u:.4f}')
                    print(f'\tavg internal p-val\t{mean_internal_pval:.6f}')
                    print(f'\tavg external p-val\t{mean_external_pval:.6f}')
                    print(f'\tavg T\t{mean_T:.4f}')
                    print(f'\tavg num tips\t{tips_mean:.0f}\t[{tips_min:.0f}-{tips_max:.0f}]' if not pd.isna(
                        tips_mean) else '\tavg num tips\tNaN')

                    print('--------FN/FP details:')
                    ddff = ddf[(ddf['result'] == 'FN') | (ddf['result'] == 'FP')]

                    if len(ddff) > 0:
                        # Convert to string and replace NaN with 'nan' for consistent output
                        internal_u_list = '\t'.join(
                            f'{float(_):.4f}' if not pd.isna(_) else 'nan' for _ in ddff['internal_u_stat'].to_list())
                        external_u_list = '\t'.join(
                            f'{float(_):.4f}' if not pd.isna(_) else 'nan' for _ in ddff['external_u_stat'].to_list())
                        evidence_list = '\t'.join(f'{_}' for _ in ddff['BDSKY_evidence'].astype(str).to_list())
                        print(f'\tinternal U stats\t{internal_u_list}')
                        print(f'\texternal U stats\t{external_u_list}')
                        print(f'\tevidence\t{evidence_list}')
                    else:
                        print('\tNo FN/FP entries for this model/data type.')

                    print('==============\n')

    df.sort_values(by=['model', 'tree/forest', 'id', 'evidence_type'], inplace=True)

    # Save the dataframe to the specified tab file
    if params.tab:
        try:
            df.to_csv(params.tab, sep='\t', index=False)
            logging.info(f"Summary table saved to {params.tab}")
        except Exception as e:
            logging.error(f"Error saving summary table to {params.tab}: {e}")
    else:
        logging.warning("No output table file specified. Table will not be saved.")